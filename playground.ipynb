{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "111dde01",
   "metadata": {},
   "source": [
    "### Goal: Summaraize text and predict what category it is out of business, entertainemnt, politics, sport and tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ed5a6ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5060189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(r\"C:\\Users\\bamilosin\\Documents\\dataset\\nlp\\summarization\\BBC News Summary\")\n",
    "articles_path = data_path / \"News Articles\"\n",
    "summaries_path = data_path / \"Summaries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eedfb7",
   "metadata": {},
   "source": [
    "expected df pattern\n",
    "\n",
    " {\n",
    "\n",
    "     \"text\": [\"a ridiculously long text\", \"another ridiculously long text\"],\n",
    "\n",
    "     \"summary\" : [\"long text\", \"another long text\"],\n",
    "\n",
    "     \"category\" : [\"normal\", \"normal\"]\n",
    "\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b819274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>TimeWarner said fourth quarter sales rose 2% t...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>The dollar has hit its highest level against t...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>Yukos' owner Menatep Group says it will ask Ro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>Rod Eddington, BA's chief executive, said the ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>Pernod has reduced the debt it took on to fund...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>BT program to beat dialler scams\\n\\nBT is intr...</td>\n",
       "      <td>BT is introducing two initiatives to help beat...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>Spam e-mails tempt net shoppers\\n\\nComputer us...</td>\n",
       "      <td>A third of them read unsolicited junk e-mail a...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>Be careful how you code\\n\\nA new European dire...</td>\n",
       "      <td>This goes to the heart of the European project...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>US cyber security chief resigns\\n\\nThe man mak...</td>\n",
       "      <td>Amit Yoran was director of the National Cyber ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>Losing yourself in online gaming\\n\\nOnline rol...</td>\n",
       "      <td>He says that in the world of online gaming suc...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               articles  \\\n",
       "0     Ad sales boost Time Warner profit\\n\\nQuarterly...   \n",
       "1     Dollar gains on Greenspan speech\\n\\nThe dollar...   \n",
       "2     Yukos unit buyer faces loan claim\\n\\nThe owner...   \n",
       "3     High fuel prices hit BA's profits\\n\\nBritish A...   \n",
       "4     Pernod takeover talk lifts Domecq\\n\\nShares in...   \n",
       "...                                                 ...   \n",
       "2220  BT program to beat dialler scams\\n\\nBT is intr...   \n",
       "2221  Spam e-mails tempt net shoppers\\n\\nComputer us...   \n",
       "2222  Be careful how you code\\n\\nA new European dire...   \n",
       "2223  US cyber security chief resigns\\n\\nThe man mak...   \n",
       "2224  Losing yourself in online gaming\\n\\nOnline rol...   \n",
       "\n",
       "                                              summaries    labels  \n",
       "0     TimeWarner said fourth quarter sales rose 2% t...  business  \n",
       "1     The dollar has hit its highest level against t...  business  \n",
       "2     Yukos' owner Menatep Group says it will ask Ro...  business  \n",
       "3     Rod Eddington, BA's chief executive, said the ...  business  \n",
       "4     Pernod has reduced the debt it took on to fund...  business  \n",
       "...                                                 ...       ...  \n",
       "2220  BT is introducing two initiatives to help beat...      tech  \n",
       "2221  A third of them read unsolicited junk e-mail a...      tech  \n",
       "2222  This goes to the heart of the European project...      tech  \n",
       "2223  Amit Yoran was director of the National Cyber ...      tech  \n",
       "2224  He says that in the world of online gaming suc...      tech  \n",
       "\n",
       "[2225 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from .txt's across all categories for articles and thier summaries\n",
    "categories = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "\n",
    "articles  = []\n",
    "label = []\n",
    "summaries = []\n",
    "\n",
    "for category in categories:\n",
    "    for file in os.listdir(articles_path / category):\n",
    "        with open(articles_path / category / file) as f:\n",
    "            file_ = f.read()\n",
    "            articles.append(file_)\n",
    "            label.append(category)\n",
    "\n",
    "        with open(summaries_path / category / file) as f:\n",
    "            file_ = f.read()\n",
    "            summaries.append(file_)\n",
    "\n",
    "\n",
    "data_dict = {\n",
    "    \"articles\" : articles,\n",
    "    \"summaries\" : summaries,\n",
    "    \"labels\" : label\n",
    "}\n",
    "\n",
    "data = pd.DataFrame(data_dict)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "66fcb764",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, stopwords):\n",
    "        super(NewsDataset, self).__init__()\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        self.data['cleaned_articles'] = self.data['articles'].apply(preprocess)\n",
    "        self.data['cleaned_summaries'] = self.data['summaries'].apply(preprocess)\n",
    "\n",
    "        self.data['cleaned_articles_tokens'] = self.data['cleaned_articles'].apply(lambda x: tokenize_text(x, tokenizer, stopwords))\n",
    "        self.data['cleaned_summaries_tokens'] = self.data['cleaned_summaries'].apply(lambda x: tokenize_text(x, tokenizer, stopwords))\n",
    "\n",
    "        # get longest article\n",
    "        self.longest_article = self.get_longest_seq(list(self.data['cleaned_articles_tokens'].values))\n",
    "        # get longest summary\n",
    "        self.longest_summary = self.get_longest_seq(list(self.data['cleaned_summaries_tokens'].values))\n",
    "\n",
    "        # article tokens list\n",
    "        self.all_tokens_articles =  list(self.data['cleaned_articles_tokens'].values)\n",
    "        # summaries tokens list\n",
    "        self.all_tokens_summaries =  list(self.data['cleaned_summaries_tokens'].values)\n",
    "        # all tokens in articles and summaries\n",
    "        self.all_tokens = all_tokens_articles + all_tokens_summaries\n",
    "        # list of unique tokens\n",
    "        self.all_tokens = sorted(list(set([token for tokens_list in self.all_tokens for token in tokens_list])))\n",
    "\n",
    "        # get vocab\n",
    "        self.vocab = create_vocab(self.all_tokens)\n",
    "\n",
    "        # pad article tokens\n",
    "        self.data['padded_articles_tokens'] = self.data['cleaned_articles_tokens'].apply(lambda x: self.pad_tokens(x, self.longest_article))\n",
    "        # pad summary tokens\n",
    "        self.data['padded_summaries_tokens'] = self.data['cleaned_summaries_tokens'].apply(lambda x: self.pad_tokens(x, self.longest_summary))\n",
    "\n",
    "        # get article indices\n",
    "        self.data[\"articles_indices\"] = self.data[\"padded_articles_tokens\"].apply(lambda x : self.get_indices(x, self.vocab))\n",
    "        # # get summary tokens\n",
    "        self.data[\"summaries_indices\"] = self.data[\"padded_summaries_tokens\"].apply(lambda x : self.get_indices(x, self.vocab))\n",
    "\n",
    "        self.label_map = {\n",
    "            \"business\": 0, \"entertainment\" : 1, \"politics\" : 2, \"sport\" : 3, \"tech\" : 4\n",
    "        }\n",
    "\n",
    "        self.data['label_idxs'] = self.data['labels'].apply(lambda x : self.get_label_indices(x, self.label_map))\n",
    "\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        lower = text.lower()\n",
    "        text = re.sub(r'!\"#$%&\\'()*+-/:;<=>?@[\\\\]^_`{|}~', '', lower)\n",
    "        # add sos and eos tokens\n",
    "        text = '<SOS>' + text + '<EOS>'\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "    def tokenize_text(self, text, tokenizer, stopwords):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        tokens = [token for token in tokens if token not in stopwords]  \n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def get_longest_seq(self, tokens_list):\n",
    "        longest_seq = 0\n",
    "\n",
    "        for tokens in tokens_list:\n",
    "            # get longest sequence\n",
    "            if len(tokens) > longest_seq:\n",
    "                longest_seq = len(tokens)\n",
    "\n",
    "        return longest_seq\n",
    "\n",
    "\n",
    "    def pad_tokens(self, tokens, longest_seq):\n",
    "        if len(tokens) < longest_seq:\n",
    "            tokens = tokens + ['<PAD>' for _ in range(longest_seq - len(tokens))]\n",
    "        return tokens\n",
    "        \n",
    "\n",
    "    def create_vocab(self, all_tokens):\n",
    "        vocab = {\n",
    "            '<UNK>' : 0,\n",
    "            '<PAD>' : 1,\n",
    "            '<SOS>' : 2,\n",
    "            '<EOS>' : 3\n",
    "        }\n",
    "\n",
    "        for token in all_tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = (len(vocab) - 1) + 1\n",
    "        \n",
    "        return vocab    \n",
    "\n",
    "    \n",
    "    def get_indices(self, tokens, vocab):\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            indices.append(vocab[token])\n",
    "\n",
    "        return indices\n",
    "    \n",
    "    def get_label_indices(self, label, label_map):\n",
    "        return label_map[label]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        article = torch.tensor(self.data.loc[idx, 'articles_indices'])\n",
    "        summary = torch.tensor(self.data.loc[idx, 'summaries_indices'])\n",
    "        label = torch.tensor(self.data.loc[idx, 'label_idxs'])\n",
    "\n",
    "        return article, summary, label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beec0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.TweetTokenizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "dataset = NewsDataset(data, tokenizer, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edfc850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    2,  3522, 34389,  ...,     1,     1,     1]),\n",
       " tensor([    2, 39718, 34122,  ...,     1,     1,     1]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
