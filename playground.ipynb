{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "111dde01",
   "metadata": {},
   "source": [
    "### Goal: Summaraize text and predict what category it is out of business, entertainemnt, politics, sport and tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5a6ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5060189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(r\"C:\\Users\\bamilosin\\Documents\\dataset\\nlp\\summarization\\BBC News Summary\")\n",
    "articles_path = data_path / \"News Articles\"\n",
    "summaries_path = data_path / \"Summaries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eedfb7",
   "metadata": {},
   "source": [
    "expected df pattern\n",
    "\n",
    " {\n",
    "\n",
    "     \"text\": [\"a ridiculously long text\", \"another ridiculously long text\"],\n",
    "\n",
    "     \"summary\" : [\"long text\", \"another long text\"],\n",
    "\n",
    "     \"category\" : [\"normal\", \"normal\"]\n",
    "\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b819274",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "\n",
    "articles  = []\n",
    "label = []\n",
    "summaries = []\n",
    "\n",
    "for category in categories:\n",
    "    for file in os.listdir(articles_path / category):\n",
    "        with open(articles_path / category / file) as f:\n",
    "            file_ = f.read()\n",
    "            articles.append(file_)\n",
    "            label.append(category)\n",
    "\n",
    "        with open(summaries_path / category / file) as f:\n",
    "            file_ = f.read()\n",
    "            summaries.append(file_)\n",
    "\n",
    "\n",
    "data_dict = {\n",
    "    \"articles\" : articles,\n",
    "    \"summaries\" : summaries,\n",
    "    \"labels\" : label\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89098b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>TimeWarner said fourth quarter sales rose 2% t...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>The dollar has hit its highest level against t...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>Yukos' owner Menatep Group says it will ask Ro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>Rod Eddington, BA's chief executive, said the ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>Pernod has reduced the debt it took on to fund...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>BT program to beat dialler scams\\n\\nBT is intr...</td>\n",
       "      <td>BT is introducing two initiatives to help beat...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>Spam e-mails tempt net shoppers\\n\\nComputer us...</td>\n",
       "      <td>A third of them read unsolicited junk e-mail a...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>Be careful how you code\\n\\nA new European dire...</td>\n",
       "      <td>This goes to the heart of the European project...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>US cyber security chief resigns\\n\\nThe man mak...</td>\n",
       "      <td>Amit Yoran was director of the National Cyber ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>Losing yourself in online gaming\\n\\nOnline rol...</td>\n",
       "      <td>He says that in the world of online gaming suc...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               articles  \\\n",
       "0     Ad sales boost Time Warner profit\\n\\nQuarterly...   \n",
       "1     Dollar gains on Greenspan speech\\n\\nThe dollar...   \n",
       "2     Yukos unit buyer faces loan claim\\n\\nThe owner...   \n",
       "3     High fuel prices hit BA's profits\\n\\nBritish A...   \n",
       "4     Pernod takeover talk lifts Domecq\\n\\nShares in...   \n",
       "...                                                 ...   \n",
       "2220  BT program to beat dialler scams\\n\\nBT is intr...   \n",
       "2221  Spam e-mails tempt net shoppers\\n\\nComputer us...   \n",
       "2222  Be careful how you code\\n\\nA new European dire...   \n",
       "2223  US cyber security chief resigns\\n\\nThe man mak...   \n",
       "2224  Losing yourself in online gaming\\n\\nOnline rol...   \n",
       "\n",
       "                                              summaries    labels  \n",
       "0     TimeWarner said fourth quarter sales rose 2% t...  business  \n",
       "1     The dollar has hit its highest level against t...  business  \n",
       "2     Yukos' owner Menatep Group says it will ask Ro...  business  \n",
       "3     Rod Eddington, BA's chief executive, said the ...  business  \n",
       "4     Pernod has reduced the debt it took on to fund...  business  \n",
       "...                                                 ...       ...  \n",
       "2220  BT is introducing two initiatives to help beat...      tech  \n",
       "2221  A third of them read unsolicited junk e-mail a...      tech  \n",
       "2222  This goes to the heart of the European project...      tech  \n",
       "2223  Amit Yoran was director of the National Cyber ...      tech  \n",
       "2224  He says that in the world of online gaming suc...      tech  \n",
       "\n",
       "[2225 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(data_dict)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97477226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft debuts security tools\n",
      "\n",
      "Microsoft is releasing tools that clean up PCs harbouring viruses and spyware.\n",
      "\n",
      "The virus-fighting program will be updated monthly and is a precursor to Microsoft releasing dedicated anti-virus software. Also being released is a software utility that will help users find and remove any spyware on their home computer. Although initially free it is thought that soon Microsoft will be charging users for the anti-spyware tool.\n",
      "\n",
      "The anti-spyware tool is available now and the anti-virus utility is expected to be available later this month. Microsoft's Windows operating system has long been a favourite of people who write computer viruses because it is so ubiquitous and has many loopholes that can be exploited. It has proved such a tempting target that there are now thought to be more than 100,000 viruses and other malicious programs in existence. Latest research suggests that new variants of viruses are being cranked out at a rate of up to 200 per week. Spyware is surreptitious software that sneaks on to home computers, often without users' knowledge. In its most benign form it just bombards users with pop-up adverts or hijacks web browser settings. The most malicious forms steal confidential information or log every keystroke that users make. Surveys have shown that most PCs are infested with spyware. Research by technology firms Earthlink and Webroot revealed that 90% of Windows machine have the malicious software on board and, on average, each one harbours 28 separate spyware programs. Before now Microsoft has left the market for PC security software to specialist firms such as Symantec, McAfee, Trend Micro and many others. It said that its virus cleaning program would not stop machines being infected nor remove the need for other anti-virus programs. On spyware freely available programs such as Ad-Aware and Spybot have become widely used by people keen to keep the latest variants at bay. Microsoft's two security tools have emerged as a result of acquisitions the company has made over the last two years. In 2003 it bought Romanian firm GeCAD Software to get hold of its anti-virus technology. In December 2004 it bought New York-based anti-spyware firm Giant Company Software. Last year Microsoft also released the SP2 upgrade for Windows XP that closed many security loopholes in the software and made it easier for people to manage their anti-virus and firewall programs.\n",
      "\n",
      "Microsoft is releasing tools that clean up PCs harbouring viruses and spyware.Also being released is a software utility that will help users find and remove any spyware on their home computer.Last year Microsoft also released the SP2 upgrade for Windows XP that closed many security loopholes in the software and made it easier for people to manage their anti-virus and firewall programs.Research by technology firms Earthlink and Webroot revealed that 90% of Windows machine have the malicious software on board and, on average, each one harbours 28 separate spyware programs.The virus-fighting program will be updated monthly and is a precursor to Microsoft releasing dedicated anti-virus software.Before now Microsoft has left the market for PC security software to specialist firms such as Symantec, McAfee, Trend Micro and many others.In 2003 it bought Romanian firm GeCAD Software to get hold of its anti-virus technology.In December 2004 it bought New York-based anti-spyware firm Giant Company Software.\n"
     ]
    }
   ],
   "source": [
    "idx = 2000\n",
    "print(data.loc[idx]['articles'])\n",
    "print(data.loc[idx]['summaries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e56930e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "# to lowercase\n",
    "# remove '\\n|\\r|\\t'\n",
    "\n",
    "def preprocess(text):\n",
    "    lower = text.lower()\n",
    "    text = re.sub(r'!\"#$%&\\'()*+-/:;<=>?@[\\\\]^_`{|}~', '', lower)\n",
    "    # add sos and eos tokens\n",
    "    text = '<SOS>' + text + '<EOS>'\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_text(text, tokenizer, stopwords):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stopwords]  \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_longest_seq(tokens_list):\n",
    "    longest_seq = 0\n",
    "\n",
    "    for tokens in tokens_list:\n",
    "        # get longest sequence\n",
    "        if len(tokens) > longest_seq:\n",
    "            longest_seq = len(tokens)\n",
    "\n",
    "    return longest_seq\n",
    "\n",
    "\n",
    "def pad_tokens(tokens, longest_seq):\n",
    "    if len(tokens) < longest_seq:\n",
    "        tokens = tokens + ['<PAD>' for _ in range(longest_seq - len(tokens))]\n",
    "    return tokens\n",
    "    \n",
    "\n",
    "def create_vocab(all_tokens):\n",
    "    vocab = {\n",
    "        '<UNK>' : 0,\n",
    "        '<PAD>' : 1,\n",
    "        '<SOS>' : 2,\n",
    "        '<EOS>' : 3\n",
    "    }\n",
    "\n",
    "    for token in all_tokens:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = (len(vocab) - 1) + 1\n",
    "    \n",
    "    return vocab    \n",
    "\n",
    "  \n",
    "def get_indices(tokens, vocab):\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        indices.append(vocab[token])\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "   \n",
    "tokenizer = nltk.TweetTokenizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53f2542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_articles'] = data['articles'].apply(preprocess)\n",
    "data['cleaned_summaries'] = data['summaries'].apply(preprocess)\n",
    "\n",
    "data['cleaned_articles_tokens'] = data['cleaned_articles'].apply(lambda x: tokenize_text(x, tokenizer, stopwords))\n",
    "data['cleaned_summaries_tokens'] = data['cleaned_summaries'].apply(lambda x: tokenize_text(x, tokenizer, stopwords))\n",
    "\n",
    "# get longest article\n",
    "longest_article = get_longest_seq(list(data['cleaned_articles_tokens'].values))\n",
    "# get longest summary\n",
    "longest_summary = get_longest_seq(list(data['cleaned_summaries_tokens'].values))\n",
    "\n",
    "# article tokens list\n",
    "all_tokens_articles =  list(data['cleaned_articles_tokens'].values)\n",
    "# summaries tokens list\n",
    "all_tokens_summaries =  list(data['cleaned_summaries_tokens'].values)\n",
    "# all tokens in articles and summaries\n",
    "all_tokens = all_tokens_articles + all_tokens_summaries\n",
    "# list of unique tokens\n",
    "all_tokens = sorted(list(set([token for tokens_list in all_tokens for token in tokens_list])))\n",
    "\n",
    "# get vocab\n",
    "vocab = create_vocab(all_tokens)\n",
    "\n",
    "# pad article tokens\n",
    "data['padded_articles_tokens'] = data['cleaned_articles_tokens'].apply(lambda x: pad_tokens(x, longest_article))\n",
    "# pad summary tokens\n",
    "data['padded_summaries_tokens'] = data['cleaned_summaries_tokens'].apply(lambda x: pad_tokens(x, longest_summary))\n",
    "\n",
    "# get article indices\n",
    "data[\"articles_indices\"] = data[\"padded_articles_tokens\"].apply(lambda x : get_indices(x, vocab))\n",
    "# # get summary tokens\n",
    "data[\"summaries_indices\"] = data[\"padded_summaries_tokens\"].apply(lambda x : get_indices(x, vocab))\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fcb764",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (3705038558.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mclass NewsDatase(Dataset):\u001b[39m\n                              ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, data tokenizer, stopwords):\n",
    "        super(self, NewsDataset).__init__()\n",
    "\n",
    "        self.data['cleaned_articles'] = self.data['articles'].apply(preprocess)\n",
    "        self.data['cleaned_summaries'] = self.data['summaries'].apply(preprocess)\n",
    "\n",
    "        self.data['cleaned_articles_tokens'] = self.data['cleaned_articles'].apply(lambda x: tokenize_text(x, tokenizer, stopwords))\n",
    "        self.data['cleaned_summaries_tokens'] = self.data['cleaned_summaries'].apply(lambda x: tokenize_text(x, tokenizer, stopwords))\n",
    "\n",
    "        # get longest article\n",
    "        self.longest_article = get_longest_seq(list(self.data['cleaned_articles_tokens'].values))\n",
    "        # get longest summary\n",
    "        self.longest_summary = get_longest_seq(list(self.data['cleaned_summaries_tokens'].values))\n",
    "\n",
    "        # article tokens list\n",
    "        self.all_tokens_articles =  list(self.data['cleaned_articles_tokens'].values)\n",
    "        # summaries tokens list\n",
    "        self.all_tokens_summaries =  list(self.data['cleaned_summaries_tokens'].values)\n",
    "        # all tokens in articles and summaries\n",
    "        self.all_tokens = all_tokens_articles + all_tokens_summaries\n",
    "        # list of unique tokens\n",
    "        self.all_tokens = sorted(list(set([token for tokens_list in all_tokens for token in tokens_list])))\n",
    "\n",
    "        # get vocab\n",
    "        vocab = create_vocab(all_tokens)\n",
    "\n",
    "        # pad article tokens\n",
    "        self.data['padded_articles_tokens'] = self.data['cleaned_articles_tokens'].apply(lambda x: pad_tokens(x, self.longest_article))\n",
    "        # pad summary tokens\n",
    "        self.data['padded_summaries_tokens'] = self.data['cleaned_summaries_tokens'].apply(lambda x: pad_tokens(x, self.longest_summary))\n",
    "\n",
    "        # get article indices\n",
    "        self.data[\"articles_indices\"] = self.data[\"padded_articles_tokens\"].apply(lambda x : get_indices(x, self.vocab))\n",
    "        # # get summary tokens\n",
    "        self.data[\"summaries_indices\"] = self.data[\"padded_summaries_tokens\"].apply(lambda x : get_indices(x, self.vocab))\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        lower = text.lower()\n",
    "        text = re.sub(r'!\"#$%&\\'()*+-/:;<=>?@[\\\\]^_`{|}~', '', lower)\n",
    "        # add sos and eos tokens\n",
    "        text = '<SOS>' + text + '<EOS>'\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "    def tokenize_text(self, text, tokenizer, stopwords):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        tokens = [token for token in tokens if token not in stopwords]  \n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def get_longest_seq(self, tokens_list):\n",
    "        longest_seq = 0\n",
    "\n",
    "        for tokens in tokens_list:\n",
    "            # get longest sequence\n",
    "            if len(tokens) > longest_seq:\n",
    "                longest_seq = len(tokens)\n",
    "\n",
    "        return longest_seq\n",
    "\n",
    "\n",
    "    def pad_tokens(self, tokens, longest_seq):\n",
    "        if len(tokens) < longest_seq:\n",
    "            tokens = tokens + ['<PAD>' for _ in range(longest_seq - len(tokens))]\n",
    "        return tokens\n",
    "        \n",
    "\n",
    "    def create_vocab(self, all_tokens):\n",
    "        vocab = {\n",
    "            '<UNK>' : 0,\n",
    "            '<PAD>' : 1,\n",
    "            '<SOS>' : 2,\n",
    "            '<EOS>' : 3\n",
    "        }\n",
    "\n",
    "        for token in all_tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = (len(vocab) - 1) + 1\n",
    "        \n",
    "        return vocab    \n",
    "\n",
    "    \n",
    "    def get_indices(self, tokens, vocab):\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            indices.append(vocab[token])\n",
    "\n",
    "        return indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beec0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
